(window.webpackJsonp=window.webpackJsonp||[]).push([[5],{173:function(a,e,t){"use strict";t.r(e);var n=t(0),s=Object(n.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var a=this,e=a.$createElement,t=a._self._c||e;return t("div",{staticClass:"content"},[t("h1",{attrs:{id:"从kafka加载数据"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#从kafka加载数据","aria-hidden":"true"}},[a._v("#")]),a._v(" 从kafka加载数据")]),a._v(" "),t("h2",{attrs:{id:"入门"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#入门","aria-hidden":"true"}},[a._v("#")]),a._v(" 入门")]),a._v(" "),t("p",[a._v("本教程讲述如何通过kafka加载数据到Druid。\n在本教程中，我们假设您已经按照快速入门中所述下载了Druid和Tranquility，并将其在本机上运行。并且您不需要事先加载数据。")]),a._v(" "),t("div",{staticClass:"tip custom-block"},[t("p",{staticClass:"custom-block-title"},[a._v("提示")]),a._v(" "),t("p",[a._v("本教程会指导如何通过kafka向Druid加载数据，但Druid还支持多种批量和流式加载数据的方法。可以通过 Loading files and Loading streams页面来学习其它更多数据加载的方法。包括 Hadoop、HTTP、Storm、Samza、Spark Streaming以及自研的JVM应用")])]),a._v(" "),t("h2",{attrs:{id:"启动kafka"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#启动kafka","aria-hidden":"true"}},[a._v("#")]),a._v(" 启动kafka")]),a._v(" "),t("p",[a._v("Apache Kafka是一个高吞吐量的消息中间件，可以和Druid配合使用。本教程中使用的是Kafka 0.9.0.0，可以通过如下指令下载kafka：")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("curl -O http://www.us.apache.org/dist/kafka/0.9.0.0/kafka_2.11-0.9.0.0.tgz\ntar -xzf kafka_2.11-0.9.0.0.tgz\ncd kafka_2.11-0.9.0.0\n")])])]),t("p",[a._v("执行如下指令启动kafka broker：")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("./bin/kafka-server-start.sh config/server.properties\n")])])]),t("p",[a._v("创建一个名称为metrics的topic用来接收数据：")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic metrics\n")])])]),t("h2",{attrs:{id:"发送示例数据"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#发送示例数据","aria-hidden":"true"}},[a._v("#")]),a._v(" 发送示例数据")]),a._v(" "),t("p",[a._v("下面就可以开始通过console producer向kafka对应的topic发送数据了！\n在Druid目录下执行如下指令：")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("bin/generate-example-metrics \n")])])]),t("p",[a._v("在kafka目录下执行：")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic metrics\n")])])]),t("p",[a._v("现在kafka-console-producer就开始等待数据的输入了，复制刚生成的示例数据并粘贴到kafka-console-producer控制台终端，回车确认。当然也可以复制更多数据到终端，或者CTRL-D退出。\n现在就可以进行数据查询了，当然也可以参考下文去加载自定义数据集。")]),a._v(" "),t("h2",{attrs:{id:"数据查询"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#数据查询","aria-hidden":"true"}},[a._v("#")]),a._v(" 数据查询")]),a._v(" "),t("p",[a._v("数据发送完成后就可以进行数据查询了，使用方法详见 supported query methods.\n加载自定义数据\n目前为止，我们已经按照Druid发布版本中的数据提取规范，将数据从kafka加载到了Druid。每一个数据提取规范都是为了特定的数据集设计的，也可以通过自定义提取规范来加载自定义数据。\n自定义数据提取规范，可以按需修改conf-quickstart/tranquility/kafka.json配置文件")]),a._v(" "),t("ul",[t("li",[a._v("dataSchema，使用的数据集名称")]),a._v(" "),t("li",[a._v("timestampSpec，哪个是时间字段")]),a._v(" "),t("li",[a._v("dimensionsSpec，哪些能作为维度字段")]),a._v(" "),t("li",[a._v("metricsSpec，哪些能作为度量进行计算")])]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v('{\n  "dataSources" : {\n    "metrics-kafka" : {\n      "spec" : {\n        "dataSchema" : {\n          "dataSource" : "metrics-kafka",\n          "parser" : {\n            "type" : "string",\n            "parseSpec" : {\n              "timestampSpec" : {\n                "column" : "timestamp",\n                "format" : "auto"\n              },\n              "dimensionsSpec" : {\n                "dimensions" : [],\n                "dimensionExclusions" : [\n                  "timestamp",\n                  "value"\n                ]\n              },\n              "format" : "json"\n            }\n          },\n          "granularitySpec" : {\n            "type" : "uniform",\n            "segmentGranularity" : "hour",\n            "queryGranularity" : "none"\n          },\n          "metricsSpec" : [\n            {\n              "type" : "count",\n              "name" : "count"\n            },\n            {\n              "name" : "value_sum",\n              "type" : "doubleSum",\n              "fieldName" : "value"\n            },\n            {\n              "fieldName" : "value",\n              "name" : "value_min",\n              "type" : "doubleMin"\n            },\n            {\n              "type" : "doubleMax",\n              "name" : "value_max",\n              "fieldName" : "value"\n            }\n          ]\n        },\n        "ioConfig" : {\n          "type" : "realtime"\n        },\n        "tuningConfig" : {\n          "type" : "realtime",\n          "maxRowsInMemory" : "100000",\n          "intermediatePersistPeriod" : "PT10M",\n          "windowPeriod" : "PT10M"\n        }\n      },\n      "properties" : {\n        "task.partitions" : "1",\n        "task.replicants" : "1",\n        "topicPattern" : "metrics"\n      }\n    }\n  },\n  "properties" : {\n    "zookeeper.connect" : "localhost",\n    "druid.discovery.curator.path" : "/druid/discovery",\n    "druid.selectors.indexing.serviceName" : "druid/overlord",\n    "commit.periodMillis" : "15000",\n    "consumer.numThreads" : "2",\n    "kafka.zookeeper.connect" : "localhost",\n    "kafka.group.id" : "tranquility-kafka"\n  }\n}\n')])])]),t("p",[a._v("下面使用网页浏览为例并将输入发送到pageviews的topic里，示例数据如下：")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v('{"time": "2000-01-01T00:00:00Z", "url": "/foo/bar", "user": "alice", "latencyMs": 32}\n')])])]),t("p",[a._v("首先创建topic")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic pageviews\n")])])]),t("p",[a._v("修改conf-quickstart/tranquility/kafka.json配置文件，修改后：")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v('{\n  "dataSources" : {\n    "metrics-kafka" : {\n      "spec" : {\n        "dataSchema" : {\n          "dataSource" : "pageviews-kafka",\n          "parser" : {\n            "type" : "string",\n            "parseSpec" : {\n              "timestampSpec" : {\n                "column" : "time",\n                "format" : "auto"\n              },\n              "dimensionsSpec" : {\n                "dimensions" : ["url", "user"],\n                "dimensionExclusions" : [\n                  "timestamp",\n                  "value"\n                ]\n              },\n              "format" : "json"\n            }\n          },\n          "granularitySpec" : {\n            "type" : "uniform",\n            "segmentGranularity" : "hour",\n            "queryGranularity" : "none"\n          },\n          "metricsSpec" : [\n            {\n              "name": "views",\n             "type": "count"\n            },\n           {\n              "name": "latencyMs", \n              "type": "doubleSum",\n              "fieldName": "latencyMs"\n            }\n          ]\n        },\n        "ioConfig" : {\n          "type" : "realtime"\n        },\n        "tuningConfig" : {\n          "type" : "realtime",\n          "maxRowsInMemory" : "100000",\n          "intermediatePersistPeriod" : "PT10M",\n          "windowPeriod" : "PT10M"\n        }\n      },\n      "properties" : {\n        "task.partitions" : "1",\n        "task.replicants" : "1",\n        "topicPattern" : "pageviews"\n      }\n    }\n  },\n  "properties" : {\n    "zookeeper.connect" : "localhost",\n    "druid.discovery.curator.path" : "/druid/discovery",\n    "druid.selectors.indexing.serviceName" : "druid/overlord",\n    "commit.periodMillis" : "15000",\n    "consumer.numThreads" : "2",\n    "kafka.zookeeper.connect" : "localhost",\n    "kafka.group.id" : "tranquility-kafka"\n  }\n}\n')])])]),t("p",[a._v("下面启动Druid的kafka提取服务：")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("bin/tranquility kafka -configFile ../druid-0.9.2/conf-quickstart/tranquility/kafka.json\n")])])]),t("p",[a._v("如果Tranquility或者kafka已经启动，可以停止并重新启动。")]),a._v(" "),t("p",[a._v("最后将数据发送到kafka的topic，以下面这些数据为例：")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v('{"time": "2000-01-01T00:00:00Z", "url": "/foo/bar", "user": "alice", "latencyMs": 32}\n{"time": "2000-01-01T00:00:00Z", "url": "/", "user": "bob", "latencyMs": 11}\n{"time": "2000-01-01T00:00:00Z", "url": "/foo/bar", "user": "bob", "latencyMs": 45}\n')])])]),t("p",[a._v("Druid流处理需要相对当前（准实时）的数据，相而言windowPeriod值控制的是更宽松的时间窗口（也就是流处理会检查数据timestamp的值，而时间窗口只关注数据接收的时间）。所以需要将2000-01-01T00:00:00Z转换为ISO8601格式的当前系统时间，你可以用以下命令转换：")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("python -c 'import datetime; print(datetime.datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\"))'\n")])])]),t("p",[a._v("更新上述JSON中的时间戳，然后将这些消息复制并粘贴到此kafka-console-producer，然后按Enter键：")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic pageviews\n")])])]),t("p",[a._v("就这样，数据应该已经保存在Druid里了，可以使用任何Druid支持的查询方式查询这些数据了。")])])}],!1,null,null,null);s.options.__file="从kafka加载数据.md";e.default=s.exports}}]);